# -*- coding: utf-8 -*-
"""SpanishPoS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18F9ueoByhwnEG-JHNlCZzsQGM_S5g6Dj
"""

!pip install transformers torch scikit-learn

import torch
from transformers import BertTokenizer, BertModel
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from collections import defaultdict

# Load mBERT
tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")
model = BertModel.from_pretrained("bert-base-multilingual-cased", output_hidden_states=True)
model.eval()

# Parse .conllu files
def parse_conllu(filepath):
    token_pos_pairs = []
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip() == "" or line.startswith("#"):
                continue
            parts = line.strip().split("\t")
            if len(parts) > 3:
                token = parts[1]
                pos = parts[3]
                token_pos_pairs.append((token, pos))
    return token_pos_pairs

# Load the data
train_data = parse_conllu("es_ancora-ud-train.conllu")
test_data = parse_conllu("es_ancora-ud-test.conllu")

# Reduce dataset size for speed (optional)
train_data = train_data[:2000]
test_data = test_data[:500]

# POS tag vocab
pos_vocab = sorted(list(set([pos for _, pos in train_data])))
pos2id = {pos: i for i, pos in enumerate(pos_vocab)}

# Extract embeddings
def get_embeddings(data):
    layer_outputs = defaultdict(list)
    labels = []

    for token, pos in data:
        inputs = tokenizer(token, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)
        hidden_states = outputs.hidden_states  # 13 layers

        for i, layer in enumerate(hidden_states):
            vector = layer[0][0].numpy()
            layer_outputs[i].append(vector)
        labels.append(pos2id[pos])

    return layer_outputs, labels

print("Extracting training embeddings...")
X_train_layers, y_train = get_embeddings(train_data)

print("Extracting test embeddings...")
X_test_layers, y_test = get_embeddings(test_data)

# Train & evaluate classifiers
layer_accuracies = []
for layer in range(13):
    clf = LogisticRegression(max_iter=200)
    clf.fit(X_train_layers[layer], y_train)
    y_pred = clf.predict(X_test_layers[layer])
    acc = accuracy_score(y_test, y_pred)
    print(f"Layer {layer}: Accuracy = {acc:.4f}")
    layer_accuracies.append(acc)

# Plot accuracy
plt.figure(figsize=(8, 5))
plt.plot(range(13), layer_accuracies, marker='o')
plt.title("Layer-wise POS Probing Accuracy on Spanish (mBERT)")
plt.xlabel("BERT Layer")
plt.ylabel("Accuracy")
plt.grid(True)
plt.show()