# -*- coding: utf-8 -*-
"""SpanishSentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11B1aCX3ayhsbpVwlke69O6mZJU3pTfjY
"""

!pip install transformers torch scikit-learn pandas

import pandas as pd
import torch
from transformers import BertTokenizer, BertModel
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from collections import defaultdict

# Load dataset
df = pd.read_csv("spanish_sentiment_sample.csv")
texts = df['text'].tolist()
labels = df['label'].tolist()

# Load mBERT
tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")
model = BertModel.from_pretrained("bert-base-multilingual-cased", output_hidden_states=True)
model.eval()

# Extract CLS token embeddings from all 13 layers
layer_outputs = defaultdict(list)

for text in texts:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    with torch.no_grad():
        hs = model(**inputs).hidden_states
    for i, layer in enumerate(hs):
        cls_vector = layer[0, 0, :].numpy()
        layer_outputs[i].append(cls_vector)

# Train/test split
X_train_layers = {k: v[:8] for k, v in layer_outputs.items()}
X_test_layers = {k: v[8:] for k, v in layer_outputs.items()}
y_train = labels[:8]
y_test = labels[8:]

# Train and evaluate classifiers per layer
accuracies = []
for layer in range(13):
    clf = LogisticRegression(max_iter=300)
    clf.fit(X_train_layers[layer], y_train)
    y_pred = clf.predict(X_test_layers[layer])
    acc = accuracy_score(y_test, y_pred)
    print(f"Layer {layer}: Accuracy = {acc:.4f}")
    accuracies.append(acc)

# Plot results
plt.plot(range(13), accuracies, marker='o')
plt.title("Layer-wise Sentiment Accuracy (mBERT, Spanish)")
plt.xlabel("Layer")
plt.ylabel("Accuracy")
plt.grid(True)
plt.show()